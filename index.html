<!DOCTYPE html>
<html lang="en">
    <head>
        <link rel="stylesheet" type="text/css" href="main.css">

        <title>Text analysis of Congressional Hearings published by US GPO</title>
    </head>
    <body>
        <h1>Establishing framework of large-scale text analysis using US GPO Congress Hearings files</h1>
        <p>
            The project's idea originated when I noticed [laughter] in some congressional hearings documents. 
            The idea of this project is to handle large-scale text processing via the following steps:
            <ul>
			  <li>Download html documents with a crawler</li>
			  <li>Read the documents with Python</li>
			  <li>Pull paragraphs with Python regular expressions and create a csv file for R</li>
			  <li>Break down the paragraphs into individual words with proper indexing</li>
			  <li>Calculate various cumulative word counts:
		            <ul>
					  <li>Per hearing</li>
					  <li>Per chamber (House/Senate)</li>
					  <li>Per committee etc.</li>
			  
			</ul>
		</p>
		
		<h1>Final graphical findings of research</h1>
        <p>The final sample included roughly 370 documents.</p>
        <p>The sample included comparable volume of words in each congress session.</p>
        <img class="full" src = "./publish/another4.png">
        
        <p>People with titles other than "Mr", "Ms" and "Mrs" have caused quite a substantial amount of laughter. Such titles were "Senator", 
	        "Chairman", "Secretary" and so on.</p>
        <img class="full" src = "./publish/another1.png">
        
        <p>When such cases are isolated, men still dominate in causing laughter in Congress.</p>
        <img class="full" src = "./publish/another2.png">
        
        <p>Even though men do have an established presence in Congress as reflected in the number of words said in hearings, women have said more words in two committees. </p>
        <img class="full" src = "./publish/another3.png">
        
		
        <h1>Working graph prototypes</h1>
        <p> While the full scale model is currently being setup, some of the initial graphs are listed below. A sample of 24 documents was used resulting in a cleaned up 400k row data frame. Full set includes 25k documents and will constitute approximately half a billion rows.</p>
        
        <p> Some general observations.</p>
        <img src = "./publish/first.png">
        <img src = "./publish/second.png">
        <img src = "./publish/third.png">
        <p> More granular data - dealing with congressmen.</p>
        <img src = "./publish/fourth.png">
        <img src = "./publish/fourth1.png">
        <p> And finally some laughter.</p>
        <img src = "./publish/sixth.png">
        <img src = "./publish/sixth1.png">
<h1>Some of ggrepel is working</h1>

        <p> Only the general stuff though - there is probably something wrong with unused levels (causes infinite recursion).</p>
        <img src = "./publish/cleaner_first.png">
        <img src = "./publish/cleaner_second.png">

        
    </body>
</html>
